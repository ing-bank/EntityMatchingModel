# Copyright (c) 2023 ING Analytics Wholesale Banking
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of
# this software and associated documentation files (the "Software"), to deal in
# the Software without restriction, including without limitation the rights to
# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
# the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from sklearn.base import TransformerMixin

from emm.helper.util import string_columns_to_pyarrow
from emm.indexing.pandas_sni import PandasSortedNeighbourhoodIndexer
from emm.loggers import Timer

if TYPE_CHECKING:
    from emm.indexing.base_indexer import BaseIndexer


def select_with_prefix(df: pd.DataFrame, cols: list[str], prefix: str) -> pd.DataFrame:
    return df[cols].rename(columns=lambda c: f"{prefix}_{c}")


class PandasCandidateSelectionTransformer(TransformerMixin):
    """Pandas middleware class that aggregates candidate pairs for possible matches."""

    def __init__(
        self,
        indexers: list[BaseIndexer],
        uid_col: str | None = None,
        carry_on_cols: list[str] | None = None,
        with_no_matches: bool | None = True,
    ) -> None:
        """Pandas middleware class that aggregates name-pair candidates for matches.

        CandidateSelectionEstimator is the second step of the PandasEntityMatching pipeline, after name preprocessing.
        The candidate selector aggregates name-pair candidates, which are generated by so-called `indexers`.
        The most important input is the list of indexers. Example Indexers are:

        - PandasCosSimIndexer()
        - PandasSortedNeighbourhoodIndexer()

        Examples:
            >>> c = PandasCandidateSelectionTransformer(
            >>>        indexers=[PandasSortedNeighbourhoodIndexer(window_length=5)],
            >>>        uid_col='uid',
            >>>        carry_on_cols='name',
            >>>    )
            >>> c.fit(ground_truth_df)
            >>> candidates = c.transform(names_df)

        See indexers under `emm.indexing` for more details on usage.

        Args:
            indexers: list of indexing objects that will be used for generating candidates
            uid_col: name of the unique id column that will be copied to the dataframe with candidates (optional)
            carry_on_cols: list of column names that should be copied to the dataframe with candidates (optional)
            with_no_matches: if true, for each name with no match add an artificial row (optional)
        """
        self.indexers = indexers
        self.uid_col = uid_col
        self.carry_on_cols = carry_on_cols
        self.with_no_matches = with_no_matches
        self.gt: pd.DataFrame | None = None

    def fit(self, X: pd.DataFrame, y: pd.Series | None = None) -> TransformerMixin:
        """Fit the indexers to ground truth names

        For example this creates TFIDF matrices for the cosine similarity indexers.

        Args:
            X: ground truth dataframe with preprocessed names.
            y: ignored.

        Returns:
            self
        """
        with Timer("CandidateSelectionTransformer.fit") as timer:
            timer.log_params({"X.shape": X.shape, "n_indexers": len(self.indexers)})

            self.gt = X
            for indexer in self.indexers:
                indexer.fit(X, y)

            timer.log_param("n", len(X))
        return self

    def fit_transform(self, X: pd.DataFrame, y: pd.Series | None = None) -> pd.DataFrame:
        """Tailored placeholder for fit_transform

        Only calls fit(gt), this avoids the unnecessary transform `gt` during `SklearnPipeline.fit_transform(gt)`.

        The sklearn Pipeline is doing fit_transform for all stages excluding the last one,
        and with supervised model the CandidateSelection stage is an intermediate step.

        Args:
            X: Pandas dataframe with names that are used to fit the indexers.
            y: ignored.

        Returns:
            Pandas dataframe processed ground truth names.
        """
        self.fit(X, y)
        # pass on processed gt
        return X

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """`transform` matches `X` dataset to the previously fitted ground truth.

        Args:
            X: Pandas dataframe with preprocessed names that should be matched

        Returns:
            Pandas dataframe with the candidate matches returned by indexers. Each row contains single pair of candidates.
                Columns `gt_uid`, `uid` contains index value from ground truth and X.
                Optionally id column (specified by `self.uid_col`) and carry on columns (specified by `self.carry_on_cols`)
                are copied from gt/X dataframes with the prefixes: `gt_` or `.
                Any additional columns calculated by indexers are also preserved (i.e. score).
        """
        if self.gt is None:
            msg = "model is not fitted yet"
            raise ValueError(msg)

        with Timer("CandidateSelectionTransformer.transform") as timer:
            timer.log_param("n_indexers", len(self.indexers))

            multiple_indexers = len(self.indexers) > 1
            candidates: pd.DataFrame | None = None
            indexers_map = {}
            for i, indexer in enumerate(self.indexers):
                current: pd.DataFrame = indexer.transform(X, multiple_indexers=multiple_indexers)
                current = current.set_index(["gt_uid", "uid"], drop=True, verify_integrity=True)
                score_col = f"score_{i}"
                rank_col = f"rank_{i}"
                current = current.rename(columns={"score": score_col, "rank": rank_col})
                if multiple_indexers:
                    c = indexer.column_prefix()
                    current[c] = 1
                    indexers_map[c] = (score_col, indexer)

                if candidates is None:
                    candidates = current
                else:
                    # We can have overlapping with verbose=True, when we have multiple cosine indexers
                    # each indexer has the following: explain, gt_explain, explain_match
                    overlapping_columns = set(candidates.columns) & set(current.columns)
                    candidates = candidates.join(current, how="outer", lsuffix="", rsuffix="_current")
                    for col in overlapping_columns:
                        col_curr = f"{col}_current"
                        # these explain columns are list, so we can just concatenate them:
                        candidates[col] = candidates[col] + candidates[col_curr]
                        candidates = candidates.drop(columns=[col_curr])
            assert candidates is not None

            # admin to indicate columns with matches are found/missing per indexer
            if multiple_indexers:
                for c in indexers_map:
                    missing = candidates[candidates[c].isnull()]
                    if len(missing) > 0:
                        candidates.loc[missing.index, c] = 0
                    candidates[c] = candidates[c].astype("int8")

            timer.log_param("n_cands", len(candidates))
            timer.label("add columns")

            # move [gt_uid, uid] from index to columns
            candidates = candidates.reset_index(drop=False)

            if self.with_no_matches:
                # for all entries in X without candidate pairs create "artificial" row with gt_uid=NO_MATCH_ID
                # (replaced by None at the end of the function)
                # those "artificial" rows are need by the supervised layer for entity matching
                # we use NO_MATCH_ID instead of None to avoid problems with join (it fails on None values in the join column)
                NO_MATCH_ID = -1
                assert NO_MATCH_ID not in self.gt.index
                not_matched = pd.DataFrame({"uid": X.index.difference(candidates["uid"])})
                not_matched["gt_uid"] = NO_MATCH_ID
                if multiple_indexers:
                    for c in indexers_map:
                        not_matched[c] = 0
                candidates = pd.concat([candidates, not_matched], ignore_index=True, sort=False)

            if self.uid_col is not None:
                candidates = candidates.join(select_with_prefix(self.gt, [self.uid_col], "gt"), on="gt_uid", how="left")
                if self.uid_col in X.columns:
                    candidates = candidates.join(X[[self.uid_col]], on="uid", how="left")

            if self.carry_on_cols:
                candidates = candidates.join(
                    select_with_prefix(self.gt, [c for c in self.carry_on_cols if c in self.gt.columns], "gt"),
                    on="gt_uid",
                    how="left",
                )
                candidates = candidates.join(X[[c for c in self.carry_on_cols if c in X.columns]], on="uid", how="left")

            if self.with_no_matches:
                # change gt_uid column to nullable integer
                candidates["gt_uid"] = candidates["gt_uid"].replace({NO_MATCH_ID: np.nan}).astype("Int64")
                # change gt_entity_id to either nullable integer or nullable string.
                candidates["gt_entity_id"] = candidates["gt_entity_id"].convert_dtypes()

            timer.log_param("n", len(X))

        return candidates

    def increase_window_by_one_step(self) -> None:
        """Utility function for negative sample creation during training"""
        for indexer in self.indexers:
            indexer.increase_window_by_one_step()

    def decrease_window_by_one_step(self) -> None:
        """Utility function for negative sample creation during training"""
        for indexer in self.indexers:
            indexer.decrease_window_by_one_step()

    def _set_sni_ground_truth(self) -> None:
        """Set ground truth for SNI indexers.

        This is needed if GT has not been persisted at serialization of SNI indexers,
        and these then are reloaded from disk. If so, update the GT from here for proper running.
        """
        for indexer in self.indexers:
            if isinstance(indexer, PandasSortedNeighbourhoodIndexer) and not indexer.store_ground_truth:
                indexer.fit(self.gt)

    def _reset_sni_ground_truth(self) -> None:
        """Delete ground truth for SNI indexers.

        This is needed to ensure GT is NOT persisted at serialization of SNI indexers.
        """
        for indexer in self.indexers:
            if isinstance(indexer, PandasSortedNeighbourhoodIndexer) and not indexer.store_ground_truth:
                indexer.gt = None

    def _convert_ground_truth_to_pyarrow(self) -> None:
        """Helper function for loading GT from disk"""
        if self.gt is not None:
            self.gt = string_columns_to_pyarrow(self.gt)
